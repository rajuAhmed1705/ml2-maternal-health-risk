---
title: "Machine Learning 2: Maternal Health Risk Classification"
author: "Aisha, Mufaddal, Raju Ahmed"
date: "`r Sys.Date()`"
output:
  pdf_document:
    toc: true
    toc_depth: 2
    number_sections: true
    fig_caption: true
    latex_engine: xelatex
bibliography: references.bib
geometry: margin=0.75in
header-includes:
  - \usepackage{float}
  - \floatplacement{figure}{H}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = FALSE,
  warning = FALSE,
  message = FALSE,
  fig.align = "center",
  fig.width = 6,
  fig.height = 4,
  out.width = "85%",
  cache = FALSE
)

# Set seed for reproducibility
set.seed(123)
```

# Introduction

## Problem Statement

Maternal mortality remains a critical global health challenge, particularly in developing regions. Early identification of high-risk pregnancies enables timely medical interventions that can save lives. This project develops and compares machine learning models to predict maternal health risk as a **binary classification task** (High Risk vs. Not High Risk) based on vital health indicators collected during pregnancy.

**Rationale for Binary Classification:** The original dataset contains three ordinal risk levels (low, mid, high). Since ordinal relationships are not optimally captured by standard multi-class classifiers, we aggregate mid and low risk into a single "Not High Risk" category. This binary framing directly addresses the clinical question: *"Is this pregnancy high-risk and requiring immediate attention?"*

## Dataset Description

The Maternal Health Risk dataset was collected from hospitals and community clinics in rural Bangladesh through an IoT-based risk monitoring system [@UCI_maternal]. The dataset contains health records that can be used to predict pregnancy risk levels.

```{r load-packages}
# Load required packages
library(tidyverse)      # Data manipulation and visualization
library(caret)          # ML framework and model training
library(randomForest)   # Random Forest implementation
library(e1071)          # SVM implementation
library(corrplot)       # Correlation visualization
library(GGally)         # Pair plots
library(knitr)          # Tables
library(kableExtra)     # Enhanced tables
library(gridExtra)      # Multiple plots
library(iml)            # Interpretable ML
library(pdp)            # Partial dependence plots
library(lime)           # Local explanations
library(pROC)           # ROC curves
library(rpart)          # Decision Trees
library(rpart.plot)     # Decision Tree visualization
```

```{r load-data}
# Load the dataset
data <- read.csv("Maternal Health Risk Data Set.csv")

# Display dataset structure
cat("Dataset Dimensions:", nrow(data), "observations,", ncol(data), "variables\n")
```

**Table 1: Variable Description**

| Variable | Type | Description | Range |
|----------|------|-------------|-------|
| Age | Integer | Age of pregnant woman (years) | 10-70 |
| SystolicBP | Integer | Systolic blood pressure (mmHg) | 70-160 |
| DiastolicBP | Integer | Diastolic blood pressure (mmHg) | 49-100 |
| BS | Numeric | Blood sugar level (mmol/L) | 6.0-19.0 |
| BodyTemp | Numeric | Body temperature (°F) | 98-103 |
| HeartRate | Integer | Heart rate (bpm) | 7-90 |
| RiskLevel | Factor | Target: High Risk vs. Not High Risk | 2 classes |

# Exploratory Data Analysis

## Data Quality Check

```{r missing-values}
# Check for missing values
missing_summary <- data.frame(
  Variable = names(data),
  Missing = sapply(data, function(x) sum(is.na(x))),
  Percentage = sapply(data, function(x) round(sum(is.na(x))/length(x)*100, 2))
)
kable(missing_summary, caption = "Missing Values Summary") %>%
  kable_styling(bootstrap_options = c("striped", "hover"), full_width = FALSE)
```

The dataset has **no missing values**, eliminating the need for imputation strategies.

## Outlier Detection and Handling

```{r outlier-detection}
# Check for physiologically implausible values
cat("HeartRate range:", min(data$HeartRate), "-", max(data$HeartRate), "\n")
cat("Observations with HeartRate < 30:", sum(data$HeartRate < 30), "\n")

# Identify outlier rows
outlier_rows <- which(data$HeartRate < 30)
cat("Outlier row indices:", outlier_rows, "\n")
cat("Outlier HeartRate values:", data$HeartRate[outlier_rows], "\n")
```

**Issue Identified:** Two observations have HeartRate = 7 bpm, which is physiologically impossible (normal resting heart rate is 60-100 bpm). These are clearly data entry errors.

**Decision:** Remove these 2 observations (~0.2% of data) as they represent erroneous values, not extreme but valid measurements.

```{r remove-outliers}
# Remove physiologically impossible HeartRate values
data_clean <- data[data$HeartRate >= 30, ]
cat("Original observations:", nrow(data), "\n")
cat("After outlier removal:", nrow(data_clean), "\n")
cat("Observations removed:", nrow(data) - nrow(data_clean), "\n")
```

## Target Variable Distribution

```{r target-distribution, fig.cap="Distribution of Risk Levels (Binary)"}
# Convert to Binary Classification: High Risk vs Not High Risk
# Original: low risk, mid risk, high risk -> Binary: NotHighRisk, HighRisk
data_clean$RiskLevel <- ifelse(data_clean$RiskLevel == "high risk",
                                "HighRisk", "NotHighRisk")
data_clean$RiskLevel <- factor(data_clean$RiskLevel,
                                levels = c("NotHighRisk", "HighRisk"))

# Class distribution
class_dist <- table(data_clean$RiskLevel)
class_prop <- prop.table(class_dist)

# Create summary table
class_summary <- data.frame(
  RiskLevel = names(class_dist),
  Count = as.numeric(class_dist),
  Percentage = round(as.numeric(class_prop) * 100, 1)
)
kable(class_summary, caption = "Binary Target Variable Distribution") %>%
  kable_styling(bootstrap_options = c("striped", "hover"), full_width = FALSE)

# Visualize
ggplot(data_clean, aes(x = RiskLevel, fill = RiskLevel)) +
  geom_bar(alpha = 0.8) +
  geom_text(stat = "count", aes(label = after_stat(count)), vjust = -0.5) +
  scale_fill_manual(values = c("NotHighRisk" = "#2ecc71",
                                "HighRisk" = "#e74c3c")) +
  labs(title = "Binary Classification: High Risk vs. Not High Risk",
       x = "Risk Level", y = "Count") +
  theme_minimal() +
  theme(legend.position = "none")
```

The binary classification task has a class imbalance (~27% HighRisk, ~73% NotHighRisk), which we address using stratified sampling.

## Descriptive Statistics

```{r descriptive-stats}
# Detailed statistics by risk level
desc_stats <- data_clean %>%
  group_by(RiskLevel) %>%
  summarise(
    n = n(),
    Age_mean = round(mean(Age), 1),
    Age_sd = round(sd(Age), 1),
    SystolicBP_mean = round(mean(SystolicBP), 1),
    DiastolicBP_mean = round(mean(DiastolicBP), 1),
    BS_mean = round(mean(BS), 2),
    BodyTemp_mean = round(mean(BodyTemp), 2),
    HeartRate_mean = round(mean(HeartRate), 1)
  )

kable(desc_stats, caption = "Descriptive Statistics by Risk Level", format = "latex", booktabs = TRUE) %>%
  kable_styling(latex_options = c("striped", "scale_down"), font_size = 9)
```

## Distribution of Predictor Variables

```{r boxplots, fig.height=5, fig.cap="Box Plots of Predictor Variables by Risk Level"}
# Reshape data for faceted plotting
data_long <- data_clean %>%
  pivot_longer(cols = c(Age, SystolicBP, DiastolicBP, BS, BodyTemp, HeartRate),
               names_to = "Variable", values_to = "Value")

ggplot(data_long, aes(x = RiskLevel, y = Value, fill = RiskLevel)) +
  geom_boxplot(alpha = 0.7) +
  facet_wrap(~Variable, scales = "free_y", ncol = 3) +
  scale_fill_manual(values = c("NotHighRisk" = "#2ecc71",
                                "HighRisk" = "#e74c3c")) +
  labs(title = "Predictors by Risk Level (Binary)", x = "", y = "") +
  theme_minimal() +
  theme(legend.position = "none",
        axis.text.x = element_text(angle = 45, hjust = 1, size = 7))
```

**Key Observations:**

- **Blood Sugar (BS):** Strong discriminator - high-risk patients have notably higher BS levels
- **Systolic BP:** High-risk patients tend to have elevated systolic blood pressure
- **Age:** Older patients show higher risk levels on average
- **Body Temperature:** Some high-risk cases show elevated temperature (fever)

## Correlation Analysis

```{r correlation, fig.height=4, fig.cap="Correlation Matrix of Predictor Variables"}
# Compute correlation matrix
cor_matrix <- cor(data_clean[, 1:6])

# Visualize
corrplot(cor_matrix, method = "color", type = "upper",
         addCoef.col = "black", number.cex = 0.7,
         tl.col = "black", tl.srt = 45, tl.cex = 0.8,
         mar = c(0, 0, 1, 0))
```

**Correlation Findings:**

- Systolic and Diastolic BP show moderate positive correlation (expected physiologically)
- Most other predictors show weak correlations, suggesting independent information
- No severe multicollinearity issues detected

# Mathematical Overview of ML Methods

## Random Forest

Random Forest is an ensemble of decision trees using bootstrap aggregating (bagging) with feature randomization [@breiman2001random]. Each tree is trained on a bootstrap sample, and at each split, only $m = \sqrt{p}$ random features are considered. Final predictions use majority voting: $\hat{y} = \text{mode}\{T_b(x)\}_{b=1}^{B}$.

**Key Hyperparameters:** `ntree` (number of trees), `mtry` (features per split), `nodesize` (minimum leaf size).

**Gini Impurity** measures split quality: $G(t) = 1 - \sum_{k=1}^{K} p_k^2$

## Decision Tree

Decision Trees recursively partition the feature space using binary splits [@james2021introduction]. At each node, the algorithm selects the feature and threshold that best separates the classes. For classification, splits are evaluated using **Gini Impurity** or **Information Gain**.

**Gini Impurity:** $G(t) = 1 - \sum_{k=1}^{K} p_k^2$ where $p_k$ is the proportion of class $k$ at node $t$.

**Key Hyperparameters:** `cp` (complexity parameter for pruning), `maxdepth` (maximum tree depth), `minsplit` (minimum observations for split), `minbucket` (minimum observations in leaf).

**Advantages:** Highly interpretable, handles non-linear relationships, no feature scaling required.

## Support Vector Machine (SVM)

SVM finds the optimal hyperplane maximizing the margin between classes [@james2021introduction]. The soft-margin formulation with cost parameter $C$ handles non-separable data:
$$\min_{\beta, \beta_0, \xi} \frac{1}{2}||\beta||^2 + C\sum_{i=1}^{n}\xi_i \quad \text{s.t. } y_i(\beta^T x_i + \beta_0) \geq 1 - \xi_i$$

**Kernel Trick** enables non-linear boundaries. We use the RBF kernel: $K(x_i, x_j) = \exp(-\gamma ||x_i - x_j||^2)$

**Hyperparameters:** $C$ (cost/regularization), $\gamma$ (kernel width). Binary classification uses direct optimization.

# Model Fitting and Comparison

## Data Splitting

```{r data-splitting}
set.seed(123)

# Stratified 80/20 split (training + CV / test)
train_index <- createDataPartition(data_clean$RiskLevel, p = 0.8, list = FALSE)
train_data <- data_clean[train_index, ]
test_data <- data_clean[-train_index, ]

cat("Training set:", nrow(train_data), "observations\n")
cat("Test set:", nrow(test_data), "observations\n")

# Verify stratification
cat("\nClass proportions in training data:\n")
print(round(prop.table(table(train_data$RiskLevel)) * 100, 1))
cat("\nClass proportions in test data:\n")
print(round(prop.table(table(test_data$RiskLevel)) * 100, 1))
```

## Cross-Validation Setup

```{r cv-setup}
# 10-fold cross-validation control for binary classification
cv_control <- trainControl(
  method = "cv",
  number = 10,
  classProbs = TRUE,
  summaryFunction = twoClassSummary,
  savePredictions = "final"
)
```

## Random Forest Model

### Hyperparameter Tuning

```{r rf-tuning}
set.seed(123)

# Define tuning grid
rf_grid <- expand.grid(
  mtry = c(2, 3, 4, 5)  # sqrt(6) ≈ 2.4, so test around this value
)

# Train with cross-validation (use ROC for binary classification)
rf_model <- train(
  RiskLevel ~ .,
  data = train_data,
  method = "rf",
  trControl = cv_control,
  tuneGrid = rf_grid,
  ntree = 500,
  importance = TRUE,
  metric = "ROC"
)

# Display results
print(rf_model)
```

```{r rf-tuning-plot, fig.cap="Random Forest Hyperparameter Tuning Results"}
plot(rf_model, main = "Random Forest: mtry Tuning")
```

### Best Random Forest Model

```{r rf-best}
cat("Best mtry:", rf_model$bestTune$mtry, "\n")
cat("Best CV AUC-ROC:", round(max(rf_model$results$ROC), 4), "\n")

# Final model
rf_final <- rf_model$finalModel
print(rf_final)
```

## Decision Tree Model

### Hyperparameter Tuning

```{r dt-tuning}
set.seed(123)

# Define tuning grid for Decision Tree
dt_grid <- expand.grid(
  cp = c(0.001, 0.01, 0.05, 0.1)  # Complexity parameter
)

# Train with cross-validation
dt_model <- train(
  RiskLevel ~ .,
  data = train_data,
  method = "rpart",
  trControl = cv_control,
  tuneGrid = dt_grid,
  metric = "ROC"
)

print(dt_model)
```

```{r dt-tuning-plot, fig.cap="Decision Tree Hyperparameter Tuning Results"}
plot(dt_model, main = "Decision Tree: Complexity Parameter Tuning")
```

### Best Decision Tree Model

```{r dt-best}
cat("Best cp:", dt_model$bestTune$cp, "\n")
cat("Best CV AUC-ROC:", round(max(dt_model$results$ROC), 4), "\n")

# Final model
dt_final <- dt_model$finalModel
```

### Decision Tree Visualization

```{r dt-tree-plot, fig.height=5, fig.cap="Decision Tree Structure"}
# Plot the decision tree
rpart.plot(dt_final,
           type = 4,
           extra = 104,
           box.palette = c("#2ecc71", "#e74c3c"),
           fallen.leaves = TRUE,
           main = "Decision Tree for Maternal Health Risk")
```

The decision tree provides a highly interpretable model. Each node shows the split condition, and the path from root to leaf represents the decision rules for classification.

## Support Vector Machine Model

### Feature Scaling

```{r svm-scaling}
# Scale features using training data parameters
preProcValues <- preProcess(train_data[, 1:6], method = c("center", "scale"))

# Apply scaling
train_scaled <- predict(preProcValues, train_data)
test_scaled <- predict(preProcValues, test_data)

cat("Scaling parameters (means):\n")
print(round(preProcValues$mean, 2))
cat("\nScaling parameters (std devs):\n")
print(round(preProcValues$std, 2))
```

### Kernel Comparison

```{r kernel-comparison}
set.seed(123)

# Compare kernels using cross-validation
kernels <- c("linear", "radial", "polynomial")
kernel_results <- data.frame(Kernel = kernels, Accuracy = NA, Kappa = NA)

for (i in seq_along(kernels)) {
  set.seed(123)

  if (kernels[i] == "polynomial") {
    tune_result <- tune(svm, RiskLevel ~ ., data = train_scaled,
                        kernel = kernels[i],
                        ranges = list(cost = c(1, 10), degree = c(2, 3)),
                        tunecontrol = tune.control(cross = 10))
  } else {
    tune_result <- tune(svm, RiskLevel ~ ., data = train_scaled,
                        kernel = kernels[i],
                        ranges = list(cost = c(0.1, 1, 10)),
                        tunecontrol = tune.control(cross = 10))
  }

  kernel_results$Accuracy[i] <- round((1 - tune_result$best.performance) * 100, 2)
}

kable(kernel_results, caption = "SVM Kernel Comparison (10-fold CV)") %>%
  kable_styling(bootstrap_options = c("striped", "hover"), full_width = FALSE)

best_kernel <- kernel_results$Kernel[which.max(kernel_results$Accuracy)]
cat("\nBest kernel:", best_kernel, "\n")
```

### Hyperparameter Tuning for RBF Kernel

```{r svm-tuning}
set.seed(123)

# Define tuning grid for RBF kernel
svm_grid <- expand.grid(
  C = c(0.1, 1, 10, 100),
  sigma = c(0.01, 0.1, 0.5, 1)  # sigma = 1/(2*gamma)
)

# Train with cross-validation (use ROC for binary classification)
svm_model <- train(
  RiskLevel ~ .,
  data = train_scaled,
  method = "svmRadial",
  trControl = cv_control,
  tuneGrid = svm_grid,
  metric = "ROC"
)

print(svm_model)
```

```{r svm-tuning-plot, fig.cap="SVM Hyperparameter Tuning Results"}
plot(svm_model, main = "SVM RBF: Cost and Sigma Tuning")
```

### Best SVM Model

```{r svm-best}
cat("Best C (Cost):", svm_model$bestTune$C, "\n")
cat("Best sigma:", svm_model$bestTune$sigma, "\n")
cat("Best CV AUC-ROC:", round(max(svm_model$results$ROC), 4), "\n")
```

## Model Comparison on Test Data

### Predictions

```{r test-predictions}
# Random Forest predictions
rf_pred <- predict(rf_model, newdata = test_data)

# Decision Tree predictions
dt_pred <- predict(dt_model, newdata = test_data)

# SVM predictions (on scaled test data)
svm_pred <- predict(svm_model, newdata = test_scaled)
```

### Confusion Matrices

```{r confusion-matrices, fig.height=4, fig.cap="Confusion Matrices for All Models"}
# Random Forest confusion matrix
rf_cm <- confusionMatrix(rf_pred, test_data$RiskLevel)

# Decision Tree confusion matrix
dt_cm <- confusionMatrix(dt_pred, test_data$RiskLevel)

# SVM confusion matrix
svm_cm <- confusionMatrix(svm_pred, test_scaled$RiskLevel)

# Display
cat("=== RANDOM FOREST ===\n")
print(rf_cm$table)

cat("\n=== DECISION TREE ===\n")
print(dt_cm$table)

cat("\n=== SUPPORT VECTOR MACHINE ===\n")
print(svm_cm$table)
```

### Performance Metrics Comparison

```{r metrics-comparison}
# Extract key metrics for binary classification
# For binary classification, positive class is second level (HighRisk)
metrics_comparison <- data.frame(
  Metric = c("Accuracy", "Kappa", "Sensitivity (Recall)", "Specificity",
             "Precision (PPV)", "NPV", "F1 Score"),
  RandomForest = c(
    round(rf_cm$overall["Accuracy"] * 100, 2),
    round(rf_cm$overall["Kappa"], 3),
    round(rf_cm$byClass["Sensitivity"] * 100, 2),
    round(rf_cm$byClass["Specificity"] * 100, 2),
    round(rf_cm$byClass["Pos Pred Value"] * 100, 2),
    round(rf_cm$byClass["Neg Pred Value"] * 100, 2),
    round(rf_cm$byClass["F1"] * 100, 2)
  ),
  DecisionTree = c(
    round(dt_cm$overall["Accuracy"] * 100, 2),
    round(dt_cm$overall["Kappa"], 3),
    round(dt_cm$byClass["Sensitivity"] * 100, 2),
    round(dt_cm$byClass["Specificity"] * 100, 2),
    round(dt_cm$byClass["Pos Pred Value"] * 100, 2),
    round(dt_cm$byClass["Neg Pred Value"] * 100, 2),
    round(dt_cm$byClass["F1"] * 100, 2)
  ),
  SVM = c(
    round(svm_cm$overall["Accuracy"] * 100, 2),
    round(svm_cm$overall["Kappa"], 3),
    round(svm_cm$byClass["Sensitivity"] * 100, 2),
    round(svm_cm$byClass["Specificity"] * 100, 2),
    round(svm_cm$byClass["Pos Pred Value"] * 100, 2),
    round(svm_cm$byClass["Neg Pred Value"] * 100, 2),
    round(svm_cm$byClass["F1"] * 100, 2)
  )
)

kable(metrics_comparison, caption = "Binary Classification Metrics on Test Data") %>%
  kable_styling(bootstrap_options = c("striped", "hover"), full_width = FALSE) %>%
  row_spec(3, background = "#ffebee")  # Highlight sensitivity (critical for high-risk detection)
```

### Visual Comparison

```{r metrics-barplot, fig.cap="Model Performance Comparison"}
# Prepare data for plotting (binary metrics) - all 3 models
metrics_plot_data <- data.frame(
  Metric = rep(c("Accuracy", "Sensitivity", "Specificity", "F1 Score"), 3),
  Model = rep(c("Random Forest", "Decision Tree", "SVM"), each = 4),
  Value = c(
    rf_cm$overall["Accuracy"] * 100,
    rf_cm$byClass["Sensitivity"] * 100,
    rf_cm$byClass["Specificity"] * 100,
    rf_cm$byClass["F1"] * 100,
    dt_cm$overall["Accuracy"] * 100,
    dt_cm$byClass["Sensitivity"] * 100,
    dt_cm$byClass["Specificity"] * 100,
    dt_cm$byClass["F1"] * 100,
    svm_cm$overall["Accuracy"] * 100,
    svm_cm$byClass["Sensitivity"] * 100,
    svm_cm$byClass["Specificity"] * 100,
    svm_cm$byClass["F1"] * 100
  )
)

ggplot(metrics_plot_data, aes(x = Metric, y = Value, fill = Model)) +
  geom_bar(stat = "identity", position = "dodge", alpha = 0.8) +
  geom_text(aes(label = round(Value, 1)),
            position = position_dodge(width = 0.9), vjust = -0.5, size = 2.5) +
  scale_fill_manual(values = c("Random Forest" = "#3498db",
                                "Decision Tree" = "#27ae60",
                                "SVM" = "#9b59b6")) +
  labs(title = "Binary Classification Performance",
       y = "Percentage (%)", x = "") +
  theme_minimal() +
  theme(legend.position = "bottom") +
  ylim(0, 105)
```

### ROC Curve Analysis

```{r roc-curves, fig.cap="ROC Curves for Model Comparison"}
# Get probability predictions
rf_probs_test <- predict(rf_model, newdata = test_data, type = "prob")
dt_probs_test <- predict(dt_model, newdata = test_data, type = "prob")
svm_probs_test <- predict(svm_model, newdata = test_scaled, type = "prob")

# Create ROC objects
roc_rf <- roc(test_data$RiskLevel, rf_probs_test$HighRisk, levels = c("NotHighRisk", "HighRisk"))
roc_dt <- roc(test_data$RiskLevel, dt_probs_test$HighRisk, levels = c("NotHighRisk", "HighRisk"))
roc_svm <- roc(test_scaled$RiskLevel, svm_probs_test$HighRisk, levels = c("NotHighRisk", "HighRisk"))

# Calculate AUC
auc_rf <- round(auc(roc_rf), 4)
auc_dt <- round(auc(roc_dt), 4)
auc_svm <- round(auc(roc_svm), 4)

# Plot ROC curves
plot(roc_rf, col = "#3498db", lwd = 2, main = "ROC Curve Comparison")
plot(roc_dt, col = "#27ae60", lwd = 2, add = TRUE)
plot(roc_svm, col = "#9b59b6", lwd = 2, add = TRUE)
legend("bottomright",
       legend = c(paste0("Random Forest (AUC = ", auc_rf, ")"),
                  paste0("Decision Tree (AUC = ", auc_dt, ")"),
                  paste0("SVM (AUC = ", auc_svm, ")")),
       col = c("#3498db", "#27ae60", "#9b59b6"), lwd = 2)
abline(a = 0, b = 1, lty = 2, col = "gray")

cat("\nAUC-ROC Comparison:\n")
cat("Random Forest AUC:", auc_rf, "\n")
cat("Decision Tree AUC:", auc_dt, "\n")
cat("SVM AUC:", auc_svm, "\n")
```

# Interpretable Machine Learning (XAI)

## Feature Importance

### Random Forest Feature Importance

```{r rf-importance, fig.height=3, fig.cap="Random Forest Feature Importance"}
# Extract importance from final model
rf_importance <- importance(rf_final)
rf_imp_df <- data.frame(
  Feature = rownames(rf_importance),
  MeanDecreaseGini = rf_importance[, "MeanDecreaseGini"],
  MeanDecreaseAccuracy = rf_importance[, "MeanDecreaseAccuracy"]
)
rf_imp_df <- rf_imp_df[order(-rf_imp_df$MeanDecreaseGini), ]

ggplot(rf_imp_df, aes(x = reorder(Feature, MeanDecreaseGini), y = MeanDecreaseGini)) +
  geom_bar(stat = "identity", fill = "#3498db", alpha = 0.8) +
  coord_flip() +
  labs(title = "RF: Feature Importance (Gini)", x = "", y = "Importance") +
  theme_minimal()
```

### SVM Feature Importance

```{r svm-importance, fig.height=3, fig.cap="SVM Permutation Feature Importance"}
predictor_svm <- Predictor$new(
  model = svm_model, data = train_scaled[, 1:6],
  y = train_scaled$RiskLevel, type = "prob"
)
set.seed(123)
svm_importance <- FeatureImp$new(predictor_svm, loss = "ce", n.repetitions = 10)
svm_imp_df <- svm_importance$results

ggplot(svm_imp_df, aes(x = reorder(feature, importance), y = importance)) +
  geom_bar(stat = "identity", fill = "#9b59b6", alpha = 0.8) +
  coord_flip() +
  labs(title = "SVM: Feature Importance (Permutation)", x = "", y = "Importance") +
  theme_minimal()
```

Both models rank **Blood Sugar (BS)** as the most important feature, followed by **SystolicBP** and **Age**. This consistency across different model types strengthens our confidence in these findings.

## Partial Dependence Plots

```{r pdp-setup}
# Create predictor for Random Forest (binary: probability of HighRisk)
predictor_rf <- Predictor$new(
  model = rf_model,
  data = train_data[, 1:6],
  y = train_data$RiskLevel,
  type = "prob",
  class = "HighRisk"
)
```

```{r pdp-bs, fig.height=3.5, fig.cap="PDP: Blood Sugar Effect on High Risk Probability"}
pdp_bs_rf <- FeatureEffect$new(predictor_rf, feature = "BS", method = "pdp")
plot(pdp_bs_rf) +
  labs(title = "Effect of Blood Sugar on High Risk Probability",
       y = "P(High Risk)") +
  theme_minimal()
```

```{r pdp-sbp, fig.height=3.5, fig.cap="PDP: Systolic BP Effect on High Risk Probability"}
pdp_sbp_rf <- FeatureEffect$new(predictor_rf, feature = "SystolicBP", method = "pdp")
plot(pdp_sbp_rf) +
  labs(title = "Effect of Systolic BP on High Risk Probability",
       y = "P(High Risk)") +
  theme_minimal()
```

## Local Explanations (LIME)

### Setup LIME Explainer

```{r lime-setup}
# Create LIME explainer for Random Forest
lime_explainer_rf <- lime(train_data[, 1:6], rf_model)

# Create LIME explainer for SVM
lime_explainer_svm <- lime(train_scaled[, 1:6], svm_model)
```

### Select Cases for Explanation

```{r lime-cases}
# Find interesting test cases for binary classification
# 1. A correctly classified high-risk case
# 2. A not-high-risk case
# 3. A borderline/misclassified case (if any)

# Get predictions with probabilities
rf_probs <- predict(rf_model, newdata = test_data, type = "prob")

# Find indices
high_risk_idx <- which(test_data$RiskLevel == "HighRisk" & rf_pred == "HighRisk")[1:2]
not_high_risk_idx <- which(test_data$RiskLevel == "NotHighRisk" & rf_pred == "NotHighRisk")[1]
misclassified_idx <- which(rf_pred != test_data$RiskLevel)[1]

selected_cases <- na.omit(c(high_risk_idx, not_high_risk_idx, misclassified_idx))
cat("Selected test case indices:", selected_cases, "\n")
cat("Actual labels:", as.character(test_data$RiskLevel[selected_cases]), "\n")
cat("Predicted labels:", as.character(rf_pred[selected_cases]), "\n")
```

### LIME Explanations

```{r lime-rf, fig.height=5, fig.cap="LIME Explanations for Selected Test Cases"}
# Generate explanations using lime::explain
set.seed(123)
lime_explanation_rf <- lime::explain(
  x = test_data[selected_cases[1:2], 1:6],
  explainer = lime_explainer_rf,
  n_labels = 1,
  n_features = 4
)

# Plot
lime::plot_features(lime_explanation_rf) +
  labs(title = "LIME: Local Explanations (Random Forest)") +
  theme_minimal()
```

## SHAP Values (Shapley)

### SHAP for Individual Predictions

```{r shap-individual, fig.cap="SHAP Values for a High-Risk Case"}
# SHAP for first high-risk case
set.seed(123)
shapley_rf <- Shapley$new(predictor_rf, x.interest = test_data[high_risk_idx[1], 1:6])

plot(shapley_rf) +
  labs(title = paste("SHAP Values for Test Case", high_risk_idx[1],
                     "(Actual:", test_data$RiskLevel[high_risk_idx[1]], ")")) +
  theme_minimal()
```

# Conclusions

**Summary:** We framed maternal health risk prediction as a **binary classification task** (High Risk vs. Not High Risk), addressing the ordinal nature of the original 3-class labels. The dataset required minimal preprocessing (2 outliers removed). We compared three machine learning models: Random Forest, Decision Tree, and SVM.

**Key Findings:**

- All three models achieve strong performance in detecting high-risk pregnancies
- Blood sugar (BS) and systolic blood pressure are the strongest predictors across all models
- Decision Tree provides the most interpretable model with explicit decision rules
- Random Forest offers the best overall performance as an ensemble method
- SVM achieves competitive results with proper hyperparameter tuning

**Key XAI Insights:**

- Higher blood sugar strongly increases high-risk probability (PDP shows clear threshold)
- Systolic BP above 130-140 mmHg indicates elevated risk
- Decision Tree visualization provides clear clinical decision pathways
- LIME explanations provide case-by-case interpretability for clinical review

**Clinical Recommendations:**

- Use sensitivity as the primary metric (minimizing false negatives for patient safety)
- Blood sugar and blood pressure should be closely monitored during pregnancy
- Decision Tree can serve as an interpretable screening tool for non-technical staff
- Random Forest/SVM can be used for more accurate predictions when interpretability is less critical

**Limitations:** Dataset size (~1,000 observations), geographic scope (rural Bangladesh), limited feature set (6 predictors).

# References

<div id="refs"></div>
