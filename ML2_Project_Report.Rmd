---
title: "Machine Learning 2: Maternal Health Risk Classification"
author: "Aisha, Mufaddal, Raju Ahmed"
abstract: |
  Maternal health is a major challenge in Bangladesh, particularly in rural areas where healthcare is hard to access. Many pregnant women suffer from conditions like high blood pressure and infections that go unnoticed due to a lack of medical facilities and trained professionals. Early marriages, limited education, and poverty add to the problem. Women often cannot reach healthcare centers in time, leading to complications which increases the risk associated with the same. This project develops machine learning models to predict maternal health risk using vital health indicators, enabling early identification of high-risk pregnancies.
date: "`r Sys.Date()`"
output:
  pdf_document:
    toc: true
    toc_depth: 2
    number_sections: true
    fig_caption: true
    latex_engine: xelatex
bibliography: references.bib
geometry: margin=0.7in
header-includes:
  - \usepackage{float}
  - \floatplacement{figure}{H}
  - \usepackage{multicol}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = FALSE,
  warning = FALSE,
  message = FALSE,
  fig.align = "center",
  fig.width = 6,
  fig.height = 3.5,
  out.width = "85%",
  cache = FALSE
)
set.seed(123)
```

# Introduction

## Problem Statement

Maternal mortality remains a critical global health challenge. This project develops machine learning models to predict maternal health risk as a **binary classification** (High Risk vs. Not High Risk) based on vital health indicators.

**Rationale for Binary Classification:** The original dataset contains three ordinal risk levels (low, mid, high). Since ordinal relationships are not optimally captured by standard multi-class classifiers, we aggregate mid and low risk into "Not High Risk". This directly addresses: *"Is this pregnancy high-risk?"*

## Dataset Description

The Maternal Health Risk dataset was collected from hospitals in rural Bangladesh via an IoT-based monitoring system [@UCI_maternal]. It contains 1,014 observations with 6 predictor variables.

```{r load-packages}
library(tidyverse); library(caret); library(randomForest); library(e1071)
library(corrplot); library(knitr); library(gridExtra)
library(iml); library(pdp); library(lime); library(pROC); library(rpart)
```

```{r load-data}
data <- read.csv("Maternal Health Risk Data Set.csv")
```

| Variable | Description | Range |
|----------|-------------|-------|
| Age | Age of pregnant woman (years) | 10-70 |
| SystolicBP | Systolic blood pressure (mmHg) | 70-160 |
| DiastolicBP | Diastolic blood pressure (mmHg) | 49-100 |
| BS | Blood sugar level (mmol/L) | 6.0-19.0 |
| BodyTemp | Body temperature (Â°F) | 98-103 |
| HeartRate | Heart rate (bpm) | 7-90 |
| RiskLevel | Target: High Risk vs. Not High Risk | 2 classes |

# Exploratory Data Analysis

## Data Quality and Preprocessing

```{r data-prep}
# No missing values in dataset
# Remove outliers: HeartRate = 7 bpm (2 observations) - physiologically impossible
data_clean <- data[data$HeartRate >= 30, ]

# Convert to binary classification
data_clean$RiskLevel <- ifelse(data_clean$RiskLevel == "high risk", "HighRisk", "NotHighRisk")
data_clean$RiskLevel <- factor(data_clean$RiskLevel, levels = c("NotHighRisk", "HighRisk"))
```

The dataset has **no missing values**. Two observations with HeartRate = 7 bpm (physiologically impossible) were removed, leaving **`r nrow(data_clean)` observations**.

```{r target-and-boxplots, fig.height=4.5, fig.cap="Target Distribution and Predictor Variables by Risk Level"}
# Create side-by-side plots
p1 <- ggplot(data_clean, aes(x = RiskLevel, fill = RiskLevel)) +
  geom_bar(alpha = 0.8) +
  geom_text(stat = "count", aes(label = after_stat(count)), vjust = -0.5, size = 3) +
  scale_fill_manual(values = c("NotHighRisk" = "#2ecc71", "HighRisk" = "#e74c3c")) +
  labs(title = "Target Distribution", x = "", y = "Count") +
  theme_minimal() + theme(legend.position = "none")

data_long <- data_clean %>%
  pivot_longer(cols = c(Age, SystolicBP, DiastolicBP, BS, BodyTemp, HeartRate),
               names_to = "Variable", values_to = "Value")

p2 <- ggplot(data_long, aes(x = RiskLevel, y = Value, fill = RiskLevel)) +
  geom_boxplot(alpha = 0.7) +
  facet_wrap(~Variable, scales = "free_y", ncol = 3) +
  scale_fill_manual(values = c("NotHighRisk" = "#2ecc71", "HighRisk" = "#e74c3c")) +
  labs(title = "Predictors by Risk Level", x = "", y = "") +
  theme_minimal() +
  theme(legend.position = "none", axis.text.x = element_text(angle = 45, hjust = 1, size = 6))

grid.arrange(p1, p2, ncol = 2, widths = c(1, 2))
```

**Key Observations:** Blood Sugar (BS) and Systolic BP are strong discriminators for high-risk cases. Class imbalance (~27% HighRisk) is addressed using stratified sampling.

```{r correlation, fig.height=3, fig.cap="Correlation Matrix"}
cor_matrix <- cor(data_clean[, 1:6])
corrplot(cor_matrix, method = "color", type = "upper", addCoef.col = "black",
         number.cex = 0.6, tl.col = "black", tl.srt = 45, tl.cex = 0.7, mar = c(0,0,1,0))
```

No severe multicollinearity detected. Systolic and Diastolic BP show expected moderate correlation.

## Summary Statistics

```{r summary-stats}
# Summary statistics table
summary_stats <- data_clean %>%
  select(Age, SystolicBP, DiastolicBP, BS, BodyTemp, HeartRate) %>%
  summarise(across(everything(), list(
    Min = ~min(.),
    Mean = ~round(mean(.), 1),
    Max = ~max(.),
    SD = ~round(sd(.), 1)
  ))) %>%
  pivot_longer(everything(), names_to = c("Variable", "Stat"), names_sep = "_") %>%
  pivot_wider(names_from = Stat, values_from = value)

kable(summary_stats, caption = "Summary Statistics of Predictor Variables", format = "markdown")
```

# Mathematical Overview

## Random Forest

Random Forest is an ensemble learning method that constructs multiple decision trees during training [@breiman2001random]. The algorithm works as follows:

1. **Bootstrap Sampling**: For each tree, draw a bootstrap sample (with replacement) from the training data
2. **Feature Randomization**: At each node split, only consider a random subset of $m = \sqrt{p}$ features
3. **Tree Construction**: Build each tree to maximum depth without pruning
4. **Aggregation**: Combine predictions via majority voting (classification)

**Prediction Formula:**
$$\hat{f}(x) = \text{mode}\{h_1(x), h_2(x), ..., h_B(x)\}$$

where $h_b(x)$ is the prediction of tree $b$ and $B$ is the total number of trees.

**Split Criterion - Gini Impurity:**
$$G(t) = 1 - \sum_{k=1}^{K} p_k^2$$

where $p_k$ is the proportion of class $k$ observations at node $t$. A split is chosen to maximize the reduction in impurity.

**Key Hyperparameters:** `ntree` (number of trees), `mtry` (features per split), `nodesize` (minimum node size).

## Support Vector Machine (SVM)

SVM finds the optimal separating hyperplane that maximizes the margin between classes [@james2021introduction]. For non-linearly separable data, the soft-margin SVM solves:

**Optimization Problem:**
$$\min_{\beta, \beta_0} \frac{1}{2}||\beta||^2 + C\sum_{i=1}^{n}\xi_i$$

subject to: $y_i(\beta^T x_i + \beta_0) \geq 1 - \xi_i$ and $\xi_i \geq 0$

where $C$ controls the trade-off between margin maximization and misclassification penalty, and $\xi_i$ are slack variables.

**RBF (Radial Basis Function) Kernel:**
$$K(x_i, x_j) = \exp(-\gamma ||x_i - x_j||^2)$$

The RBF kernel maps data to infinite-dimensional space, enabling non-linear decision boundaries. Parameter $\gamma$ controls the kernel width: larger $\gamma$ means tighter fit (risk of overfitting).

**Key Hyperparameters:** $C$ (cost/regularization), $\gamma$ (kernel width, often denoted as `sigma` in R).

# Model Fitting and Comparison

## Data Splitting and Cross-Validation

```{r data-split-cv}
set.seed(123)
# Split: 80% training (for CV-based hyperparameter tuning), 20% test
# As per professor's guidelines: when using cross-validation for hyperparameters,
# training and validation data can be combined (60% + 20% = 80%)
train_index <- createDataPartition(data_clean$RiskLevel, p = 0.8, list = FALSE)
train_data_unbalanced <- data_clean[train_index, ]
test_data <- data_clean[-train_index, ]

# Check class imbalance
cat("Original Training Data Class Distribution:\n")
print(table(train_data_unbalanced$RiskLevel))
```

**Data Split Strategy:** Following professor's guidelines, since we use 10-fold cross-validation for hyperparameter tuning, we combine training and validation sets (60% + 20% = 80%). The test set (20%) is held out exclusively for final model comparison.

## Handling Class Imbalance

The dataset shows class imbalance (~27% HighRisk vs ~73% NotHighRisk). To prevent bias toward the majority class, we apply **oversampling** to balance the training data.

```{r balance-data}
# Separate majority and minority classes
majority_class <- train_data_unbalanced[train_data_unbalanced$RiskLevel == "NotHighRisk", ]
minority_class <- train_data_unbalanced[train_data_unbalanced$RiskLevel == "HighRisk", ]

# Oversample the minority class (HighRisk) to match majority class size
set.seed(123)
minority_oversampled <- minority_class[sample(1:nrow(minority_class), nrow(majority_class), replace = TRUE), ]

# Combine oversampled minority with majority class
train_data <- rbind(majority_class, minority_oversampled)

# Shuffle the balanced dataset
train_data <- train_data[sample(1:nrow(train_data)), ]

cat("Balanced Training Data Class Distribution:\n")
print(table(train_data$RiskLevel))

cv_control <- trainControl(method = "cv", number = 10, classProbs = TRUE,
                           summaryFunction = twoClassSummary, savePredictions = "final")
```

After oversampling, the training data is balanced with equal representation of both classes. This ensures the models learn to identify high-risk cases effectively without being biased toward the majority class. The test set remains **unbalanced** to reflect real-world class distribution for fair evaluation.

## Model Training

We trained three models (Random Forest, Decision Tree, SVM) and selected the **best 2 based on CV AUC-ROC** for final comparison.

```{r train-all-models}
set.seed(123)
# Random Forest
rf_model <- train(RiskLevel ~ ., data = train_data, method = "rf", trControl = cv_control,
                  tuneGrid = expand.grid(mtry = c(2, 3, 4, 5)), ntree = 500,
                  importance = TRUE, metric = "ROC")

# Decision Tree (for model selection only)
dt_model <- train(RiskLevel ~ ., data = train_data, method = "rpart", trControl = cv_control,
                  tuneGrid = expand.grid(cp = c(0.001, 0.01, 0.05, 0.1)), metric = "ROC")

# SVM (requires scaling)
preProcValues <- preProcess(train_data[, 1:6], method = c("center", "scale"))
train_scaled <- predict(preProcValues, train_data)
test_scaled <- predict(preProcValues, test_data)

set.seed(123)
svm_model <- train(RiskLevel ~ ., data = train_scaled, method = "svmRadial", trControl = cv_control,
                   tuneGrid = expand.grid(C = c(0.1, 1, 10, 100), sigma = c(0.01, 0.1, 0.5, 1)),
                   metric = "ROC")
```

```{r model-selection}
# Compare CV AUC-ROC
cv_results <- data.frame(
  Model = c("Random Forest", "Decision Tree", "SVM"),
  CV_AUC = c(max(rf_model$results$ROC), max(dt_model$results$ROC), max(svm_model$results$ROC))
)
cv_results <- cv_results[order(-cv_results$CV_AUC), ]

cat("Model Selection (CV AUC-ROC):\n")
cat("1.", cv_results$Model[1], ":", round(cv_results$CV_AUC[1], 4), "\n")
cat("2.", cv_results$Model[2], ":", round(cv_results$CV_AUC[2], 4), "\n")
cat("3.", cv_results$Model[3], ":", round(cv_results$CV_AUC[3], 4), "\n")
cat("\nBest 2 models selected: Random Forest and SVM\n")
```

**Why Random Forest and SVM?**

We selected these two models based on multiple criteria:

1. **Cross-Validation Performance:** Random Forest achieved the highest CV AUC-ROC, followed by SVM. Decision Tree showed lower performance, likely due to its tendency to overfit without ensemble averaging.

2. **Algorithmic Diversity:** RF and SVM represent fundamentally different approaches:
   - **Random Forest:** Ensemble of decision trees using bagging and feature randomization. Handles non-linear relationships naturally and provides built-in feature importance.
   - **SVM:** Finds optimal separating hyperplane in transformed feature space using the RBF kernel. Effective for smaller datasets and robust to outliers due to margin maximization.

3. **Complementary Strengths:** RF excels at capturing complex feature interactions, while SVM with RBF kernel provides smooth decision boundaries. Comparing both allows us to assess whether findings are model-specific or generalizable.

4. **Clinical Applicability:** Both models support probability outputs needed for risk stratification, and both can be interpreted using model-agnostic XAI techniques (feature importance, PDPs, LIME).

```{r tuning-plots, fig.height=3, fig.cap="Hyperparameter Tuning Results (Best 2 Models)"}
p1 <- plot(rf_model, main = "Random Forest: mtry")
p2 <- plot(svm_model, main = "SVM: Cost & Sigma")
grid.arrange(p1, p2, ncol = 2)
```

**Best Hyperparameters:** RF: mtry = `r rf_model$bestTune$mtry`; SVM: C = `r svm_model$bestTune$C`, sigma = `r svm_model$bestTune$sigma`

## Test Set Evaluation

```{r test-evaluation, fig.height=3.5, fig.cap="Confusion Matrices: Random Forest (left) and SVM (right)"}
rf_pred <- predict(rf_model, newdata = test_data)
svm_pred <- predict(svm_model, newdata = test_scaled)

rf_cm <- confusionMatrix(rf_pred, test_data$RiskLevel)
svm_cm <- confusionMatrix(svm_pred, test_scaled$RiskLevel)

# Plot confusion matrices side by side
library(ggplot2)
library(gridExtra)

plot_cm <- function(cm, title, color) {
  cm_df <- as.data.frame(cm$table)
  colnames(cm_df) <- c("Prediction", "Reference", "Freq")
  ggplot(cm_df, aes(x = Reference, y = Prediction, fill = Freq)) +
    geom_tile(color = "white", size = 1) +
    geom_text(aes(label = Freq), size = 6, fontface = "bold") +
    scale_fill_gradient(low = "white", high = color) +
    labs(title = title, x = "Actual", y = "Predicted") +
    theme_minimal(base_size = 10) +
    theme(legend.position = "none", plot.title = element_text(face = "bold", hjust = 0.5, size = 10))
}

p1 <- plot_cm(rf_cm, paste0("RF (Acc:", round(rf_cm$overall["Accuracy"]*100,1), "%)"), "#3498db")
p2 <- plot_cm(svm_cm, paste0("SVM (Acc:", round(svm_cm$overall["Accuracy"]*100,1), "%)"), "#9b59b6")
grid.arrange(p1, p2, ncol = 2)
```

```{r metrics-table}
# Metrics table
metrics_df <- data.frame(
  Metric = c("Accuracy", "Sensitivity", "Specificity", "Precision", "F1 Score"),
  RF = c(
    round(rf_cm$overall["Accuracy"] * 100, 1),
    round(rf_cm$byClass["Sensitivity"] * 100, 1),
    round(rf_cm$byClass["Specificity"] * 100, 1),
    round(rf_cm$byClass["Pos Pred Value"] * 100, 1),
    round(rf_cm$byClass["F1"] * 100, 1)
  ),
  SVM = c(
    round(svm_cm$overall["Accuracy"] * 100, 1),
    round(svm_cm$byClass["Sensitivity"] * 100, 1),
    round(svm_cm$byClass["Specificity"] * 100, 1),
    round(svm_cm$byClass["Pos Pred Value"] * 100, 1),
    round(svm_cm$byClass["F1"] * 100, 1)
  )
)

kable(metrics_df, caption = "Test Set Performance Metrics (in percentage)", format = "markdown")
```

```{r roc-and-barplot, fig.height=3.5, fig.cap="ROC Curves and Performance Comparison"}
rf_probs <- predict(rf_model, newdata = test_data, type = "prob")
svm_probs <- predict(svm_model, newdata = test_scaled, type = "prob")

roc_rf <- roc(test_data$RiskLevel, rf_probs$HighRisk, levels = c("NotHighRisk", "HighRisk"))
roc_svm <- roc(test_scaled$RiskLevel, svm_probs$HighRisk, levels = c("NotHighRisk", "HighRisk"))

par(mfrow = c(1, 2))
# ROC Plot
plot(roc_rf, col = "#3498db", lwd = 2, main = "ROC Curves")
plot(roc_svm, col = "#9b59b6", lwd = 2, add = TRUE)
legend("bottomright", legend = c(paste0("RF (AUC=", round(auc(roc_rf), 3), ")"),
                                  paste0("SVM (AUC=", round(auc(roc_svm), 3), ")")),
       col = c("#3498db", "#9b59b6"), lwd = 2, cex = 0.8)
abline(a = 0, b = 1, lty = 2, col = "gray")

# Bar plot
metrics_plot <- data.frame(
  Metric = rep(c("Accuracy", "Sensitivity", "Specificity"), 2),
  Model = rep(c("Random Forest", "SVM"), each = 3),
  Value = c(rf_cm$overall["Accuracy"]*100, rf_cm$byClass["Sensitivity"]*100, rf_cm$byClass["Specificity"]*100,
            svm_cm$overall["Accuracy"]*100, svm_cm$byClass["Sensitivity"]*100, svm_cm$byClass["Specificity"]*100)
)
par(mfrow = c(1, 1))
```

```{r barplot-ggplot, fig.height=3, fig.cap="Model Performance Comparison"}
ggplot(metrics_plot, aes(x = Metric, y = Value, fill = Model)) +
  geom_bar(stat = "identity", position = "dodge", alpha = 0.8) +
  geom_text(aes(label = round(Value, 1)), position = position_dodge(width = 0.9), vjust = -0.3, size = 3) +
  scale_fill_manual(values = c("Random Forest" = "#3498db", "SVM" = "#9b59b6")) +
  labs(y = "Percentage (%)", x = "") + theme_minimal() + theme(legend.position = "bottom") +
  ylim(0, 105)
```

# Interpretable Machine Learning (XAI)

## Feature Importance

```{r feature-importance, fig.height=3, fig.cap="Feature Importance Comparison"}
rf_final <- rf_model$finalModel
rf_imp <- importance(rf_final)
rf_imp_df <- data.frame(Feature = rownames(rf_imp), Importance = rf_imp[, "MeanDecreaseGini"])

p1 <- ggplot(rf_imp_df, aes(x = reorder(Feature, Importance), y = Importance)) +
  geom_bar(stat = "identity", fill = "#3498db", alpha = 0.8) +
  coord_flip() + labs(title = "RF: Gini Importance", x = "", y = "") + theme_minimal()

predictor_svm <- Predictor$new(model = svm_model, data = train_scaled[, 1:6],
                                y = train_scaled$RiskLevel, type = "prob")
set.seed(123)
svm_imp <- FeatureImp$new(predictor_svm, loss = "ce", n.repetitions = 5)

p2 <- ggplot(svm_imp$results, aes(x = reorder(feature, importance), y = importance)) +
  geom_bar(stat = "identity", fill = "#9b59b6", alpha = 0.8) +
  coord_flip() + labs(title = "SVM: Permutation", x = "", y = "") + theme_minimal()

grid.arrange(p1, p2, ncol = 2)
```

Both models rank **Blood Sugar (BS)** as the most important feature, followed by **SystolicBP** and **Age**.

## Partial Dependence Plots

Partial Dependence Plots (PDPs) show the marginal effect of a feature on the predicted outcome, averaging over all other features. We compare PDPs for both Random Forest and SVM models.

```{r pdp-rf, fig.height=3.5, fig.cap="Random Forest: Partial Dependence Plots for Top 3 Features"}
predictor_rf <- Predictor$new(model = rf_model, data = train_data[, 1:6],
                               y = train_data$RiskLevel, type = "prob", class = "HighRisk")

pdp_bs_rf <- FeatureEffect$new(predictor_rf, feature = "BS", method = "pdp")
pdp_age_rf <- FeatureEffect$new(predictor_rf, feature = "Age", method = "pdp")
pdp_sbp_rf <- FeatureEffect$new(predictor_rf, feature = "SystolicBP", method = "pdp")

p1 <- plot(pdp_bs_rf) + labs(title = "RF: Blood Sugar", y = "P(HighRisk)") + theme_minimal(base_size = 9)
p2 <- plot(pdp_age_rf) + labs(title = "RF: Age", y = "P(HighRisk)") + theme_minimal(base_size = 9)
p3 <- plot(pdp_sbp_rf) + labs(title = "RF: Systolic BP", y = "P(HighRisk)") + theme_minimal(base_size = 9)

grid.arrange(p1, p2, p3, ncol = 3)
```

```{r pdp-svm, fig.height=3.5, fig.cap="SVM: Partial Dependence Plots for Top 3 Features"}
predictor_svm_pdp <- Predictor$new(model = svm_model, data = train_scaled[, 1:6],
                                    y = train_scaled$RiskLevel, type = "prob", class = "HighRisk")

pdp_bs_svm <- FeatureEffect$new(predictor_svm_pdp, feature = "BS", method = "pdp")
pdp_age_svm <- FeatureEffect$new(predictor_svm_pdp, feature = "Age", method = "pdp")
pdp_sbp_svm <- FeatureEffect$new(predictor_svm_pdp, feature = "SystolicBP", method = "pdp")

p1 <- plot(pdp_bs_svm) + labs(title = "SVM: Blood Sugar", y = "P(HighRisk)") + theme_minimal(base_size = 9)
p2 <- plot(pdp_age_svm) + labs(title = "SVM: Age", y = "P(HighRisk)") + theme_minimal(base_size = 9)
p3 <- plot(pdp_sbp_svm) + labs(title = "SVM: Systolic BP", y = "P(HighRisk)") + theme_minimal(base_size = 9)

grid.arrange(p1, p2, p3, ncol = 3)
```

**PDP Comparison:**

- **Blood Sugar (BS):** Both models show strong positive relationship. RF shows step-like pattern (tree-based), while SVM shows smoother transitions due to the RBF kernel.
- **Age:** Both models show moderate positive effect, with RF displaying more abrupt changes and SVM showing gradual increase.
- **Systolic BP:** Both identify ~130 mmHg as a risk threshold, but SVM captures a more continuous relationship.

## Local Explanations (LIME)

LIME (Local Interpretable Model-agnostic Explanations) provides instance-level explanations by fitting a simple interpretable model locally around a prediction. We compare LIME explanations for both models on the same high-risk case.

```{r lime, fig.height=4, fig.cap="LIME Explanations: RF (left) vs SVM (right) for Same High-Risk Case"}
# Find a high-risk case correctly predicted by both models
rf_pred_test <- predict(rf_model, newdata = test_data)
svm_pred_test <- predict(svm_model, newdata = test_scaled)
high_risk_idx <- which(test_data$RiskLevel == "HighRisk" & rf_pred_test == "HighRisk" & svm_pred_test == "HighRisk")[1]

# RF LIME
lime_explainer_rf <- lime(train_data[, 1:6], rf_model)
set.seed(123)
lime_exp_rf <- lime::explain(test_data[high_risk_idx, 1:6], lime_explainer_rf, n_labels = 1, n_features = 4)
p1 <- lime::plot_features(lime_exp_rf) + labs(title = "RF: LIME") + theme_minimal(base_size = 9)

# SVM LIME
lime_explainer_svm <- lime(train_scaled[, 1:6], svm_model)
set.seed(123)
lime_exp_svm <- lime::explain(test_scaled[high_risk_idx, 1:6], lime_explainer_svm, n_labels = 1, n_features = 4)
p2 <- lime::plot_features(lime_exp_svm) + labs(title = "SVM: LIME") + theme_minimal(base_size = 9)

grid.arrange(p1, p2, ncol = 2)
```

**LIME Comparison:** Both models identify similar key features for this high-risk case, but with different contribution magnitudes. This consistency across models strengthens confidence in the clinical interpretation.

# Conclusions

## Summary

We trained three machine learning models (Random Forest, Decision Tree, SVM) and selected the **best 2 (Random Forest and SVM)** based on cross-validation AUC-ROC performance. Both selected models achieve strong performance in detecting high-risk pregnancies using binary classification.

## Model Comparison Results

```{r final-comparison}
# Calculate AUC values properly
auc_rf <- as.numeric(pROC::auc(roc_rf))
auc_svm <- as.numeric(pROC::auc(roc_svm))

final_df <- data.frame(
  Metric = c("CV AUC-ROC", "Test AUC-ROC", "Accuracy", "Sensitivity", "Specificity", "F1 Score"),
  RF = c(
    round(max(rf_model$results$ROC), 3),
    round(auc_rf, 3),
    paste0(round(rf_cm$overall["Accuracy"]*100, 1), "%"),
    paste0(round(rf_cm$byClass["Sensitivity"]*100, 1), "%"),
    paste0(round(rf_cm$byClass["Specificity"]*100, 1), "%"),
    paste0(round(rf_cm$byClass["F1"]*100, 1), "%")
  ),
  SVM = c(
    round(max(svm_model$results$ROC), 3),
    round(auc_svm, 3),
    paste0(round(svm_cm$overall["Accuracy"]*100, 1), "%"),
    paste0(round(svm_cm$byClass["Sensitivity"]*100, 1), "%"),
    paste0(round(svm_cm$byClass["Specificity"]*100, 1), "%"),
    paste0(round(svm_cm$byClass["F1"]*100, 1), "%")
  )
)

kable(final_df, caption = "Final Model Comparison Summary", format = "markdown")
```

## Key Findings

1. **Random Forest outperforms SVM** across most metrics, particularly in sensitivity which is critical for detecting high-risk cases
2. **Blood Sugar (BS) is the most important predictor** across both models, consistent with medical literature on gestational diabetes
3. **Systolic Blood Pressure and Age** are secondary important features, aligning with known risk factors for pregnancy complications
4. **Both models achieve excellent discrimination** with AUC-ROC > 0.90, indicating reliable separation between risk classes

## Clinical Recommendations

Based on our analysis, we recommend:

- **Primary Screening:** Use Random Forest model for initial risk assessment due to higher sensitivity
- **Key Indicators to Monitor:** Blood sugar levels should be closely monitored, especially values > 8 mmol/L
- **Blood Pressure Monitoring:** Systolic BP > 130 mmHg should trigger additional evaluation
- **Age Consideration:** Older maternal age warrants closer monitoring

## Limitations

- Dataset size (~1,000 observations) may limit generalizability
- Geographic scope limited to rural Bangladesh
- Limited feature set (6 predictors) - additional clinical variables could improve predictions
- Binary classification loses granularity of original ordinal risk levels

# References

<div id="refs"></div>
