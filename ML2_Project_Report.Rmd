---
title: "Machine Learning 2: Maternal Health Risk Classification"
author: "Group I: Aisha, Mufaddal, Raju Ahmed"
abstract: |
  Maternal health is a major challenge in Bangladesh, particularly in rural areas where healthcare is hard to access. Many pregnant women suffer from conditions like high blood pressure and infections that go unnoticed due to a lack of medical facilities and trained professionals. Early marriages, limited education, and poverty add to the problem. Women often cannot reach healthcare centers in time, leading to complications which increases the risk associated with the same. This project develops machine learning models to predict maternal health risk using vital health indicators, enabling early identification of high-risk pregnancies.
date: ""
output:
  pdf_document:
    toc: true
    toc_depth: 2
    number_sections: true
    fig_caption: true
    latex_engine: xelatex
bibliography: references.bib
geometry: margin=0.7in
header-includes:
  - \usepackage{float}
  - \floatplacement{figure}{H}
  - \usepackage{multicol}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = FALSE,
  warning = FALSE,
  message = FALSE,
  fig.align = "center",
  fig.width = 6,
  fig.height = 3.5,
  out.width = "85%",
  cache = FALSE
)
set.seed(123)
```

# Introduction

## Problem Statement

Maternal mortality remains a critical global health challenge. This project develops machine learning models to predict maternal health risk as a **binary classification** (High Risk vs. Not High Risk) based on vital health indicators.

**Rationale for Binary Classification:** The original dataset contains three ordinal risk levels (low, mid, high). Since ordinal relationships are not optimally captured by standard multi-class classifiers, we aggregate mid and low risk into "Not High Risk". This directly addresses: *"Is this pregnancy high-risk?"*

## Dataset Description

The Maternal Health Risk dataset was collected from hospitals in rural Bangladesh via an IoT-based monitoring system [@UCI_maternal]. It contains 1,014 observations with 6 predictor variables.

**Dataset Source:** [UCI Machine Learning Repository - Maternal Health Risk](https://archive.ics.uci.edu/dataset/863/maternal+health+risk)

```{r load-packages}
library(tidyverse); library(caret); library(randomForest); library(e1071)
library(corrplot); library(knitr); library(gridExtra)
library(iml); library(pdp); library(pROC); library(rpart)
library(fastshap); library(shapviz)
```

```{r load-data}
data <- read.csv("Maternal Health Risk Data Set.csv")
```

**Attributes Description:**

- **Age:** Age of the pregnant woman in years, ranging from teenagers to older mothers
- **SystolicBP / DiastolicBP:** Blood pressure measurements (mmHg), key indicators for hypertension-related complications like preeclampsia
- **BS (Blood Sugar):** Blood glucose level (mmol/L), important for detecting gestational diabetes
- **BodyTemp:** Body temperature (°F), elevated values may indicate infections
- **HeartRate:** Resting heart rate (bpm), abnormal values may signal cardiovascular stress
- **RiskLevel:** Target variable with three original categories (low, mid, high risk) converted to binary classification

| Variable | Description | Range |
|----------|-------------|-------|
| Age | Age of pregnant woman (years) | 10-70 |
| SystolicBP | Systolic blood pressure (mmHg) | 70-160 |
| DiastolicBP | Diastolic blood pressure (mmHg) | 49-100 |
| BS | Blood sugar level (mmol/L) | 6.0-19.0 |
| BodyTemp | Body temperature (°F) | 98-103 |
| HeartRate | Heart rate (bpm) | 7-90 |
| RiskLevel | Target: High Risk vs. Not High Risk | 2 classes |

```{r sample-data}
# Display first 5 rows of the dataset
kable(head(data, 5), caption = "Sample Data: First 5 Observations", format = "markdown")
```

# Exploratory Data Analysis

## Data Quality and Preprocessing

```{r data-prep}
# No missing values in dataset
# Remove outliers: HeartRate = 7 bpm (2 observations) - physiologically impossible
data_clean <- data[data$HeartRate >= 30, ]

# Convert to binary classification
data_clean$RiskLevel <- ifelse(data_clean$RiskLevel == "high risk", "HighRisk", "NotHighRisk")
data_clean$RiskLevel <- factor(data_clean$RiskLevel, levels = c("NotHighRisk", "HighRisk"))
```

The dataset has **no missing values**. Two observations with HeartRate = 7 bpm (physiologically impossible) were removed, leaving **`r nrow(data_clean)` observations**.

```{r target-and-boxplots, fig.height=3.2, fig.cap="Target Distribution and Predictor Variables by Risk Level"}

p1 <- ggplot(data_clean, aes(x = RiskLevel, fill = RiskLevel)) +
  geom_bar(alpha = 0.8) +
  geom_text(stat = "count", aes(label = after_stat(count)), vjust = -0.5, size = 3) +
  scale_fill_manual(values = c("NotHighRisk" = "#2ecc71", "HighRisk" = "#e74c3c")) +
  scale_y_continuous(expand = expansion(mult = c(0, 0.1))) +
  labs(title = "Target Distribution", x = "", y = "Count") +
  theme_minimal() + theme(legend.position = "none")

data_long <- data_clean %>%
  pivot_longer(cols = c(Age, SystolicBP, DiastolicBP, BS, BodyTemp, HeartRate),
               names_to = "Variable", values_to = "Value")

p2 <- ggplot(data_long, aes(x = RiskLevel, y = Value, fill = RiskLevel)) +
  geom_boxplot(alpha = 0.7) +
  facet_wrap(~Variable, scales = "free_y", ncol = 3) +
  scale_fill_manual(values = c("NotHighRisk" = "#2ecc71", "HighRisk" = "#e74c3c")) +
  labs(title = "Predictors by Risk Level", x = "", y = "") +
  theme_minimal() +
  theme(legend.position = "none", axis.text.x = element_text(angle = 45, hjust = 1, size = 6))

grid.arrange(p1, p2, ncol = 2, widths = c(1, 2))
```

**Key Observations:** From the boxplots, we can see that high-risk pregnancies tend to have elevated Blood Sugar (BS) and Systolic BP levels compared to non-high-risk cases. Since only about 27% of cases are high-risk, we use stratified sampling to ensure both classes are well-represented during training.

```{r correlation, fig.height=3, fig.cap="Correlation Matrix"}
cor_matrix <- cor(data_clean[, 1:6])
corrplot(cor_matrix, method = "color", type = "upper", addCoef.col = "black",
         number.cex = 0.6, tl.col = "black", tl.srt = 45, tl.cex = 0.7, mar = c(0,0,1,0))
```

The correlation matrix shows no severe multicollinearity among the predictors. Systolic and Diastolic blood pressure show a moderate positive correlation (r = 0.79), which is expected as both measure arterial pressure during different phases of the cardiac cycle [@franklin1999pulse].

## Summary Statistics

```{r summary-stats}
# Summary statistics table
summary_stats <- data_clean %>%
  select(Age, SystolicBP, DiastolicBP, BS, BodyTemp, HeartRate) %>%
  summarise(across(everything(), list(
    Min = ~min(.),
    Mean = ~round(mean(.), 1),
    Max = ~max(.),
    SD = ~round(sd(.), 1)
  ))) %>%
  pivot_longer(everything(), names_to = c("Variable", "Stat"), names_sep = "_") %>%
  pivot_wider(names_from = Stat, values_from = value)

kable(summary_stats, caption = "Summary Statistics of Predictor Variables", format = "markdown")
```

# Model Selection and Mathematical Overview

By looking at the target variable, this is a binary classification problem. We decided to implement Random Forest and SVM models.

## Random Forest

Random Forest is an ensemble learning method that constructs multiple decision trees during training [@breiman2001random]. For each tree in the forest, a bootstrap sample is drawn with replacement from the training data. During the construction of each tree, only a random subset of $m = \sqrt{p}$ features is considered at each node split, introducing additional randomness beyond the bootstrap sampling. Each tree is grown to maximum depth without pruning, allowing it to capture complex patterns in the data. Finally, predictions from all trees are combined through majority voting for classification tasks.

**Prediction Formula:**
$$\hat{f}(x) = \text{mode}\{h_1(x), h_2(x), ..., h_B(x)\}$$

where $h_b(x)$ is the prediction of tree $b$ and $B$ is the total number of trees (controlled by the `ntree` hyperparameter).

**Split Criterion - Gini Impurity:**
$$G(t) = 1 - \sum_{k=1}^{K} p_k^2$$

where $p_k$ is the proportion of class $k$ observations at node $t$. A split is chosen to maximize the reduction in impurity.

**Key Hyperparameters:**

- **`ntree`**: How many trees to grow. Adding more trees helps up to a point, but after around 500 the gains become negligible.

- **`mtry`**: At each split, only a random subset of features is considered. This keeps the trees different from each other. The default for classification is $\sqrt{p}$.

- **`nodesize`**: The smallest allowed leaf size. Letting leaves get very small (default is 1) can overfit the training data.

## Support Vector Machine (SVM)

A Support Vector Machine (SVM) is a supervised learning method primarily used for binary classification, though it can be extended to multiple classes. Mathematically, it works by identifying a hyperplane that serves as a boundary between different regions in a feature space. To find the "optimal" hyperplane, SVM seeks which is the boundary furthest away from the nearest data points. The distance between the boundary and the nearest points is called the margin.

**Non-Linear SVM**

When data cannot be separated by a straight line or plane, a linear boundary is insufficient. Non-linear SVMs address this by expanding the feature space.

**The Kernel Trick:** Rather than explicitly calculating the coordinates in a massive or infinite-dimensional space—which is computationally expensive—SVMs use kernel functions. This "trick" calculates the similarity between data points as if they were in a higher-dimensional space without actually performing the transformation.

**Radial Basis Function (RBF) Kernel:**
$$K(x_i, x_j) = \exp\left(-\gamma ||x_i - x_j||^2\right)$$

With respect to the dataset we have, it is not possible to separate it using a straight line, hence the Non-Linear SVM approach.

**Hyperparameter Tuning:**

- **C (Cost/Budget):** This is a general hyperparameter used in the Support Vector Classifier (soft margin) algorithm, which also applies to non-linear SVMs. It acts as a budget for the total amount of "slack" allowed for observations to fall on the wrong side of the margin or hyperplane.
- **$\gamma$ (Gamma):** Defines the influence of a single training example. A low gamma makes the influence of each training point large, while a high gamma makes it small. Note: The `caret` package uses `sigma` where $\gamma = \frac{1}{2\sigma^2}$.

**Model Evaluation:** Confusion matrix and ROC-AUC curves can help us to evaluate classification problems.

# Model Fitting and Comparison

## Data Splitting and Cross-Validation

```{r data-split-cv}
set.seed(123)

train_index <- createDataPartition(data_clean$RiskLevel, p = 0.8, list = FALSE)
train_data_unbalanced <- data_clean[train_index, ]
test_data <- data_clean[-train_index, ]

# Check class imbalance
cat("Original Training Data Class Distribution:\n")
print(table(train_data_unbalanced$RiskLevel))
```

**Data Split Strategy:** Following professor's guidelines, since we use 10-fold cross-validation for hyperparameter tuning, we combine training and validation sets (60% + 20% = 80%). The test set (20%) is held out exclusively for final model comparison.

## Handling Class Imbalance

The dataset shows class imbalance (~27% HighRisk vs ~73% NotHighRisk). To prevent bias toward the majority class, we apply **oversampling** to balance the training data.

```{r balance-data}
# Separate majority and minority classes
majority_class <- train_data_unbalanced[train_data_unbalanced$RiskLevel == "NotHighRisk", ]
minority_class <- train_data_unbalanced[train_data_unbalanced$RiskLevel == "HighRisk", ]

# Oversample the minority class (HighRisk) to match majority class size
set.seed(123)
minority_oversampled <- minority_class[sample(1:nrow(minority_class), nrow(majority_class), replace = TRUE), ]

# Combine oversampled minority with majority class
train_data <- rbind(majority_class, minority_oversampled)

# Shuffle the balanced dataset
train_data <- train_data[sample(1:nrow(train_data)), ]

cat("Balanced Training Data Class Distribution:\n")
print(table(train_data$RiskLevel))

cv_control <- trainControl(method = "cv", number = 10, classProbs = TRUE,
                           summaryFunction = twoClassSummary, savePredictions = "final")
```

After oversampling, the training data is balanced with equal representation of both classes. This ensures the models learn to identify high-risk cases effectively without being biased toward the majority class. The test set remains **unbalanced** to reflect real-world class distribution for fair evaluation.

## Model Training

We trained Random Forest and SVM models using 10-fold cross-validation with AUC-ROC as the optimization metric.

For Random Forest, we set `ntree = 500` and tuned `mtry` over values 2, 3, 4, and 5 to find the best number of features considered at each split. We kept `nodesize` at its default value (1) since `mtry` has the most impact on performance. For SVM, we tuned `C` (cost) over 0.1, 1, and 10, and `sigma` over 0.01, 0.1, and 1 to control the margin flexibility and RBF kernel width.

```{r train-all-models, echo=TRUE}
set.seed(123)
# Random Forest
rf_model <- train(RiskLevel ~ ., data = train_data, method = "rf", trControl = cv_control,
                  tuneGrid = expand.grid(mtry = c(2, 3, 4, 5)), ntree = 500,
                  importance = TRUE, metric = "ROC")

# SVM with RBF Kernel (requires scaling)
preProcValues <- preProcess(train_data[, 1:6], method = c("center", "scale"))
train_scaled <- predict(preProcValues, train_data)
test_scaled <- predict(preProcValues, test_data)

set.seed(123)
svm_model <- train(RiskLevel ~ ., data = train_scaled, method = "svmRadial", trControl = cv_control,
                   tuneGrid = expand.grid(C = c(0.1, 1, 10), sigma = c(0.01, 0.1, 1)),
                   metric = "ROC")
```

```{r model-selection}
# Compare CV AUC-ROC
cv_results <- data.frame(
  Model = c("Random Forest", "SVM"),
  CV_AUC = c(max(rf_model$results$ROC), max(svm_model$results$ROC))
)
cv_results <- cv_results[order(-cv_results$CV_AUC), ]

cat("Cross-Validation Results (AUC-ROC):\n")
cat("Random Forest:", round(max(rf_model$results$ROC), 4), "\n")
cat("SVM:", round(max(svm_model$results$ROC), 4), "\n")
```

```{r tuning-plots, fig.height=3, fig.cap="Hyperparameter Tuning Results"}
p1 <- plot(rf_model, main = "Random Forest: mtry")
p2 <- plot(svm_model, main = "SVM RBF: C and Sigma")
grid.arrange(p1, p2, ncol = 2)
```

The left plot shows how cross-validated AUC changes with different `mtry` values for Random Forest. The right plot shows SVM performance across combinations of C and sigma — each line represents a different sigma value.

**Best Hyperparameters:** RF: mtry = `r rf_model$bestTune$mtry`; SVM: C = `r svm_model$bestTune$C`, sigma = `r round(svm_model$bestTune$sigma, 3)`

## Test Set Evaluation

Both models are tested on unseen data to check how well they perform.

```{r test-evaluation}
rf_pred <- predict(rf_model, newdata = test_data)
svm_pred <- predict(svm_model, newdata = test_scaled)

rf_cm <- confusionMatrix(rf_pred, test_data$RiskLevel, positive = "HighRisk")
svm_cm <- confusionMatrix(svm_pred, test_scaled$RiskLevel, positive = "HighRisk")

# Metrics table
metrics_df <- data.frame(
  Metric = c("Accuracy", "Sensitivity", "Specificity", "Precision", "F1 Score"),
  RF = c(
    round(rf_cm$overall["Accuracy"] * 100, 1),
    round(rf_cm$byClass["Sensitivity"] * 100, 1),
    round(rf_cm$byClass["Specificity"] * 100, 1),
    round(rf_cm$byClass["Pos Pred Value"] * 100, 1),
    round(rf_cm$byClass["F1"] * 100, 1)
  ),
  SVM = c(
    round(svm_cm$overall["Accuracy"] * 100, 1),
    round(svm_cm$byClass["Sensitivity"] * 100, 1),
    round(svm_cm$byClass["Specificity"] * 100, 1),
    round(svm_cm$byClass["Pos Pred Value"] * 100, 1),
    round(svm_cm$byClass["F1"] * 100, 1)
  )
)

kable(metrics_df, caption = "Test Set Performance Metrics (in percentage)", format = "markdown")
```

```{r cm-and-roc, fig.height=2.8, fig.width=8.5, fig.cap="Confusion Matrices and ROC Curves"}
# Confusion matrix heatmaps
plot_cm <- function(cm, title) {
  cm_df <- as.data.frame(cm$table)
  colnames(cm_df) <- c("Predicted", "Actual", "n")
  ggplot(cm_df, aes(x = Actual, y = Predicted, fill = n)) +
    geom_tile(color = "white") +
    geom_text(aes(label = n), size = 3.5, fontface = "bold") +
    scale_fill_gradient(low = "#f7f7f7", high = "#2c7fb8") +
    labs(title = title) +
    theme_minimal(base_size = 8) +
    theme(legend.position = "none",
          plot.title = element_text(hjust = 0.5, size = 9),
          axis.title = element_text(size = 7))
}

p1 <- plot_cm(rf_cm, "RF Confusion Matrix")
p2 <- plot_cm(svm_cm, "SVM Confusion Matrix")

# ROC curve as ggplot
rf_probs <- predict(rf_model, newdata = test_data, type = "prob")
svm_probs <- predict(svm_model, newdata = test_scaled, type = "prob")
roc_rf <- roc(test_data$RiskLevel, rf_probs$HighRisk, levels = c("NotHighRisk", "HighRisk"))
roc_svm <- roc(test_scaled$RiskLevel, svm_probs$HighRisk, levels = c("NotHighRisk", "HighRisk"))

roc_data <- rbind(
  data.frame(FPR = 1 - roc_rf$specificities, TPR = roc_rf$sensitivities, Model = paste0("RF (AUC=", round(auc(roc_rf), 3), ")")),
  data.frame(FPR = 1 - roc_svm$specificities, TPR = roc_svm$sensitivities, Model = paste0("SVM (AUC=", round(auc(roc_svm), 3), ")"))
)

p3 <- ggplot(roc_data, aes(x = FPR, y = TPR, color = Model)) +
  geom_line(size = 1) +
  geom_abline(slope = 1, intercept = 0, linetype = "dashed", color = "gray") +
  labs(title = "ROC Curves", x = "False Positive Rate", y = "True Positive Rate") +
  scale_color_manual(values = c("#3498db", "#9b59b6")) +
  theme_minimal(base_size = 8) +
  theme(legend.position = "bottom", legend.title = element_blank(),
        plot.title = element_text(hjust = 0.5, size = 9),
        legend.text = element_text(size = 7))

grid.arrange(p1, p2, p3, ncol = 3, widths = c(1, 1, 1.3))
```

**Overfitting Check:** To ensure our models generalize well, we compare cross-validation AUC with test set AUC. Random Forest shows CV AUC of `r round(max(rf_model$results$ROC), 3)` and test AUC of `r round(auc(roc_rf), 3)`. SVM shows CV AUC of `r round(max(svm_model$results$ROC), 3)` and test AUC of `r round(auc(roc_svm), 3)`. The small gaps between CV and test performance confirm that neither model is overfitting.

Although both methods show good predictive results, machine learning models in the healthcare setting should be interpretable too. It is important to interpret the predictions to ensure trust and appropriateness. In the context of this study, interpretable machine learning methods will be used to interpret the global behavior and predictions of both models.

# Interpretable Machine Learning (XAI)

In this section, the post-hoc interpretability techniques are applied to the developed model of the Random Forest and the Support Vector Machine. Global feature importance and global feature effects are examined. Local interpretability is also explored.

## Feature Importance

```{r feature-importance, fig.height=3, fig.cap="Feature Importance Comparison"}
rf_final <- rf_model$finalModel
rf_imp <- importance(rf_final)
rf_imp_df <- data.frame(Feature = rownames(rf_imp), Importance = rf_imp[, "MeanDecreaseGini"])

p1 <- ggplot(rf_imp_df, aes(x = reorder(Feature, Importance), y = Importance)) +
  geom_bar(stat = "identity", fill = "#3498db", alpha = 0.8) +
  coord_flip() + labs(title = "RF: Gini Importance", x = "", y = "") + theme_minimal()

predictor_svm <- Predictor$new(model = svm_model, data = train_scaled[, 1:6],
                                y = train_scaled$RiskLevel, type = "prob")
set.seed(123)
svm_imp <- FeatureImp$new(predictor_svm, loss = "ce", n.repetitions = 5)

p2 <- ggplot(svm_imp$results, aes(x = reorder(feature, importance), y = importance)) +
  geom_bar(stat = "identity", fill = "#9b59b6", alpha = 0.8) +
  coord_flip() + labs(title = "SVM: Permutation", x = "", y = "") + theme_minimal()

grid.arrange(p1, p2, ncol = 2)
```

Both models rank **Blood Sugar (BS)** as the most important feature, followed by **SystolicBP** and **Age**.

## Partial Dependence Plots

Partial Dependence Plots (PDPs) show the average effect of a feature on the predicted outcome. The PDP value for a feature value $s^*$ is:

$$\hat{f}_S(s^*) = \frac{1}{n} \sum_{i=1}^{n} f(S = s^*, C_i)$$

where $C_i$ are the values of all other features for observation $i$.

```{r pdp-rf, fig.height=3, fig.cap="Partial Dependence Plots for Top 3 Features (Random Forest)"}
predictor_rf <- Predictor$new(model = rf_model, data = train_data[, 1:6],
                               y = train_data$RiskLevel, type = "prob", class = "HighRisk")

pdp_bs_rf <- FeatureEffect$new(predictor_rf, feature = "BS", method = "pdp")
pdp_age_rf <- FeatureEffect$new(predictor_rf, feature = "Age", method = "pdp")
pdp_sbp_rf <- FeatureEffect$new(predictor_rf, feature = "SystolicBP", method = "pdp")

p1 <- plot(pdp_bs_rf) + labs(title = "Blood Sugar", y = "P(HighRisk)") + theme_minimal(base_size = 9)
p2 <- plot(pdp_age_rf) + labs(title = "Age", y = "P(HighRisk)") + theme_minimal(base_size = 9)
p3 <- plot(pdp_sbp_rf) + labs(title = "Systolic BP", y = "P(HighRisk)") + theme_minimal(base_size = 9)

grid.arrange(p1, p2, p3, ncol = 3)
```

The PDPs show that higher Blood Sugar and Systolic BP values increase the probability of high-risk classification. Age shows a moderate positive effect, with risk increasing for older patients.

```{=latex}
\clearpage
```

## Local Explanations (SHAP)

SHAP (SHapley Additive exPlanations) explains individual predictions using Shapley values from game theory. The Shapley value represents the average marginal contribution of a feature across all possible feature combinations.

```{r shap, fig.height=3, fig.cap="SHAP Waterfall Plots for a High-Risk Case"}
library(fastshap)
library(shapviz)

# Find a high-risk case correctly predicted by RF
rf_pred_test <- predict(rf_model, newdata = test_data)
high_risk_idx <- which(test_data$RiskLevel == "HighRisk" & rf_pred_test == "HighRisk")[1]

# Prediction function for SHAP
pred_fun <- function(object, newdata) {
  predict(object, newdata = newdata, type = "prob")[, "HighRisk"]
}

# Calculate SHAP values
set.seed(123)
shap_values <- fastshap::explain(
  rf_model,
  X = train_data[, 1:6],
  newdata = test_data[high_risk_idx, 1:6, drop = FALSE],
  pred_wrapper = pred_fun,
  nsim = 50
)

# Create shapviz object and plot
baseline <- mean(predict(rf_model, newdata = train_data, type = "prob")[, "HighRisk"])
sv <- shapviz(shap_values, X = test_data[high_risk_idx, 1:6, drop = FALSE], baseline = baseline)
sv_waterfall(sv, row_id = 1) +
  labs(title = "SHAP Waterfall: Why This Case is High-Risk") +
  theme_minimal(base_size = 10)
```

The waterfall plot shows how each feature contributes to pushing the prediction away from the baseline (average prediction). Features pushing right increase the high-risk probability, while those pushing left decrease it.

# Conclusions

## Summary

We trained Random Forest and SVM models for maternal health risk classification. Both models achieve strong performance in detecting high-risk pregnancies using binary classification.

## Model Comparison Results

```{r final-comparison}
# Calculate AUC values properly
auc_rf <- as.numeric(pROC::auc(roc_rf))
auc_svm <- as.numeric(pROC::auc(roc_svm))

final_df <- data.frame(
  Metric = c("CV AUC-ROC", "Test AUC-ROC", "Accuracy", "Sensitivity", "Specificity", "F1 Score"),
  RF = c(
    round(max(rf_model$results$ROC), 3),
    round(auc_rf, 3),
    paste0(round(rf_cm$overall["Accuracy"]*100, 1), "%"),
    paste0(round(rf_cm$byClass["Sensitivity"]*100, 1), "%"),
    paste0(round(rf_cm$byClass["Specificity"]*100, 1), "%"),
    paste0(round(rf_cm$byClass["F1"]*100, 1), "%")
  ),
  SVM = c(
    round(max(svm_model$results$ROC), 3),
    round(auc_svm, 3),
    paste0(round(svm_cm$overall["Accuracy"]*100, 1), "%"),
    paste0(round(svm_cm$byClass["Sensitivity"]*100, 1), "%"),
    paste0(round(svm_cm$byClass["Specificity"]*100, 1), "%"),
    paste0(round(svm_cm$byClass["F1"]*100, 1), "%")
  )
)

kable(final_df, caption = "Final Model Comparison Summary", format = "markdown")
```

## Key Findings

1. **Random Forest outperforms SVM** across most metrics, particularly in sensitivity which is critical for detecting high-risk cases
2. **Blood Sugar (BS) is the most important predictor** across both models, consistent with medical literature on gestational diabetes
3. **Systolic Blood Pressure and Age** are secondary important features, aligning with known risk factors for pregnancy complications
4. **Both models achieve excellent discrimination** with AUC-ROC > 0.90, indicating reliable separation between risk classes

## Limitations

The dataset size (~1,000 observations) may limit generalizability, and the geographic scope is limited to rural Bangladesh. The feature set contains only 6 predictors, so additional clinical variables could improve predictions. Additionally, binary classification loses the granularity of the original ordinal risk levels.

# References

<div id="refs"></div>
